{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6d32920-c6f8-4df6-91a7-925640a9920a"
   },
   "source": [
    "# Introduction - Customer Churn Prediction notebook\n",
    "In this notebook, we illustrate how you can train a model for Churn Prediction using PySpark. After training the model, you step through the instructions to deploy the model using Watson Machine Learning.\n",
    "\n",
    "This notebook is a variation of the original notebook reference in this github repo: https://github.com/elenalowery/cpd4_demo/blob/master/assets/jupyterlab/Predict_Customer_Churn_CPD4.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "8b4212ee-4116-4a47-a586-f0997df1ec53"
   },
   "source": [
    "## Package installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57a4e88e-7792-4ef9-b8c2-c96cd25e7da8"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40ffac85-88dc-4462-a43a-da40b0047eaa"
   },
   "outputs": [],
   "source": [
    "# install required Python modules\n",
    "!pip install --upgrade pyspark==3.0.3 --no-cache | tail -n 1\n",
    "!pip install lime --no-cache | tail -n 1\n",
    "!pip install SciPy --no-cache | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4fe0e5c-dead-4c2a-a5fc-de179245bdf0"
   },
   "source": [
    "# Model building and deployment <a name=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9c44abc-9abb-4e6c-86bc-cb02dd7e8bb8"
   },
   "source": [
    "In this section you will learn how to train Spark MLLib model and next deploy it as web-service using Watson Machine Learning service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b69186c1-b491-40a7-a657-4ca112e82209"
   },
   "source": [
    "## Load the training data \n",
    "\n",
    "Modify the next cell to reference the name of your joined customer data set. In the previous workshop instructions, we name the dataset CUSTOMER_DATA_ready which is what the next cell references. If you called your data set something else like CUSTOMER_DATA_ready_1, then change the name in the next cell to CUSTOMER_DATA_ready_1.\n",
    "\n",
    "Alternatively, you can always pull the actual data link from the platform by following these steps:\n",
    "- Click in the next cell to insert the code to import the training dataset.\n",
    "- Click the **Find and add data** icon in the top right, find the data set you'd like to import (for example, CUSTOMER_DATA_ready) into this notebook and click **Insert to code** drop down and select **pandas DataFrame (depr...)**\n",
    "\n",
    "<font color='red'>DO NOTE select the **pandas DataFrame** option but rather the **pandas DataFrame (depr...)** option</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7268324c49e841b2a6fceba03cb8985d"
   },
   "outputs": [],
   "source": [
    "# Click here to insert code to import training datasetfrom ibm_watson_studio_lib import access_project_or_space\n",
    "# Import dataset into a pandas DataFrame\n",
    "## Sample inserted code (Note that the name of your dataframe may be different)\n",
    "##from ibm_watson_studio_lib import access_project_or_space\n",
    "##wslib = access_project_or_space()\n",
    "\n",
    "##import pandas as pd\n",
    "\n",
    "##df_data_1 = pd.read_csv(wslib.mount.get_data_path('CUSTOMER_DATA_ready'))\n",
    "##df_data_1.head()from ibm_watson_studio_lib import access_project_or_space\n",
    "from ibm_watson_studio_lib import access_project_or_space\n",
    "wslib = access_project_or_space()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_data_1 = pd.read_csv(wslib.mount.get_data_path('CUSTOMER_DATA_ready'))\n",
    "df_data_1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7a7b9f92-0fdb-4eb9-9c56-3f56653e08c5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a PySpark DataFrame from the pandas DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "# Provide the name of the pandas DataFrame from the previous cell (should be of the format df_data_<some_number>)\n",
    "pandasDFname=df_data_1\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sparkDF=spark.createDataFrame(pandasDFname)\n",
    "sparkDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84a9f61c-d459-4063-8b42-8c74d6fed26e"
   },
   "source": [
    "## Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08482041-a3cb-4649-9faa-ba434680a545",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sparkDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f82e8b88-d2f7-47a8-910c-86ee276db604",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of records: \" + str(sparkDF.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7478be73-75c1-4f83-9a13-3b807a7b3925"
   },
   "source": [
    "## Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c210dca2-86ee-4a2d-bb00-907359668466",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark_df = sparkDF\n",
    "# Split the labeled data into a training set and a test set\n",
    "(train_data, test_data) = spark_df.randomSplit([0.8, 0.2], 24)\n",
    "\n",
    "# Provide a target name for your churn model\n",
    "MODEL_NAME = \"Churn Model\"\n",
    "# Provide a target name for your churn model deployment\n",
    "DEPLOYMENT_NAME = \"Churn Deployment\"\n",
    "\n",
    "print(\"Number of records for training: \" + str(train_data.count()))\n",
    "print(\"Number of records for evaluation: \" + str(test_data.count()))\n",
    "\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "001138a9-3891-4b23-a4f6-716bfc09a7cb"
   },
   "source": [
    "The code below creates a Random Forest Classifier with Spark, setting up string indexers for the categorical features and the label column. Finally, this notebook creates a pipeline including the indexers and the model, and does an initial Area Under ROC evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3e9dfabb-15fc-4ea0-aaeb-b58c1cc5ec51",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline, Model\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "features = [x for x in spark_df.columns if x != 'CHURN']\n",
    "# Specify the categorical features\n",
    "categorical_features = ['PAYMETHOD', 'LOCALBILLTYPE', 'LONGDISTANCEBILLTYPE', 'GENDER', 'STATUS', 'CAROWNER']\n",
    "# Index the categorical feature so each string value is replaced with an integer\n",
    "categorical_num_features = [x + '_IX' for x in categorical_features]\n",
    "si_list = [StringIndexer(inputCol=x, outputCol=y) for x, y in zip(categorical_features, categorical_num_features)]\n",
    "va_features = VectorAssembler(inputCols=categorical_num_features + [x for x in features if x not in categorical_features], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a42978de-e7a6-4c3c-a075-98f555ddc8c8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Index the label column\n",
    "si_label = StringIndexer(inputCol=\"CHURN\", outputCol=\"label\").fit(spark_df)\n",
    "label_converter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=si_label.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99a07071-6748-4927-955e-86a463c55167",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "# train a Random Forect Classifier\n",
    "classifier = RandomForestClassifier(featuresCol=\"features\")\n",
    "pipeline = Pipeline(stages= si_list + [si_label, va_features, classifier, label_converter])\n",
    "\n",
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8a29125b-03b6-4565-8993-056acf9ff0a6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = model.transform(test_data)\n",
    "evaluatorDT = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",  metricName='areaUnderROC')\n",
    "area_under_curve = evaluatorDT.evaluate(predictions)\n",
    "\n",
    "evaluatorDT = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",  metricName='areaUnderPR')\n",
    "area_under_PR = evaluatorDT.evaluate(predictions)\n",
    "#default evaluation is areaUnderROC\n",
    "print(\"areaUnderROC = %g\" % area_under_curve, \"areaUnderPR = %g\" % area_under_PR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8800524-71b7-4193-9f3a-24e92d23a17f"
   },
   "outputs": [],
   "source": [
    "# extra code: evaluate more metrics by exporting them into pandas and numpy\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = predictions.toPandas()['prediction']\n",
    "y_pred = ['T' if pred == 1.0 else 'F' for pred in y_pred]\n",
    "y_test = test_data.toPandas()['CHURN']\n",
    "print(classification_report(y_test, y_pred, target_names=['T', 'F']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e27918b3-79da-4acf-b56d-7d7141b42b0c"
   },
   "source": [
    "## Publish the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c744359c-5462-4c4d-a83b-c9bc7b340299"
   },
   "source": [
    "In this section, the notebook uses Watson Machine Learning to save the model (including the pipeline) to the WML instance. Previous versions of the model are removed so that the notebook can be run again, resetting all data for another demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "526fa45a0b0447368988dcb204db10fd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "cpdtoken=os.environ['USER_ACCESS_TOKEN']\n",
    "wml_credentials = {\n",
    "\"token\": cpdtoken,\n",
    "\"instance_id\" : \"openshift\",\n",
    "\"url\": os.environ['RUNTIME_ENV_APSX_URL'],\n",
    "\"version\": \"4.0\"\n",
    "}\n",
    "\n",
    "from ibm_watson_machine_learning import APIClient\n",
    "wml_client = APIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6d902ff3-e276-41d6-98ef-109377511f4b"
   },
   "outputs": [],
   "source": [
    "def getSpaceIDwml(wml_client,space_name):\n",
    "    spaces = wml_client.spaces.get_details()['resources'];\n",
    "    try:\n",
    "        spaceList = next(item for item in spaces if item['entity']['name']==space_name)\n",
    "        spaceID = spaceList['metadata']['id']\n",
    "    except:\n",
    "        spaceID = -1\n",
    "    return spaceID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40e8202948814f419acc9a28496eab34"
   },
   "outputs": [],
   "source": [
    "def createSpacewml(wml_client,space_name):\n",
    "    spaces = wml_client.spaces.get_details()['resources'];\n",
    "    for space in spaces:\n",
    "        if space['entity']['name'] ==space_name:\n",
    "            print(\"Deployment space with name\",space_name,\"already exists . .\")\n",
    "            return space['metadata']['id']\n",
    "    print(\"\\nCreating a new deployment space -\",space_name)\n",
    "    # create the space\n",
    "    space_meta_data = {\n",
    "        wml_client.spaces.ConfigurationMetaNames.NAME : space_name\n",
    "    }\n",
    "\n",
    "    stored_space_details = wml_client.spaces.store(space_meta_data)\n",
    "    space_id = stored_space_details['metadata']['id']\n",
    "    i=0\n",
    "    while(True):\n",
    "        stored_space_details=wml_client.spaces.get_details(space_id)\n",
    "        status=stored_space_details['entity']['status']['state']\n",
    "        print(\"i: \", i, \" status: \", status)\n",
    "        if status == 'active':\n",
    "            break\n",
    "        time.sleep(1)\n",
    "        i = i+1\n",
    "    return space_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7d1f93f-d7d1-4eeb-9755-02050d488836"
   },
   "outputs": [],
   "source": [
    "# Associate Watson Machine Learning with a specific space\n",
    "space_name='churnUATspace'\n",
    "space_id=getSpaceIDwml(wml_client,space_name)\n",
    "if space_id == -1:\n",
    "    space_id = createSpacewml(wml_client,space_name)\n",
    "print('space id: ', space_id)\n",
    "wml_client.set.default_space(space_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45893978c72045868efa13ccf94da8d1"
   },
   "outputs": [],
   "source": [
    "software_spec_uid = wml_client.software_specifications.get_id_by_name(\"spark-mllib_3.2\")\n",
    "print(\"Software Specification ID: {}\".format(software_spec_uid))\n",
    "model_props = {\n",
    "        wml_client._models.ConfigurationMetaNames.NAME:\"{}\".format(MODEL_NAME),\n",
    "        wml_client._models.ConfigurationMetaNames.TYPE: \"mllib_3.2\",\n",
    "        wml_client._models.ConfigurationMetaNames.SOFTWARE_SPEC_UID: software_spec_uid,\n",
    "        #wml_client._models.ConfigurationMetaNames.TRAINING_DATA_REFERENCES: training_data_references,\n",
    "        wml_client._models.ConfigurationMetaNames.LABEL_FIELD: \"CHURN\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ae6042b95fd49448ffb5253c45e88f4"
   },
   "outputs": [],
   "source": [
    "def deleteExistingModelsSameName(wml_client,model_name):\n",
    "    stored_models=wml_client.repository.get_model_details()\n",
    "    stored_models_details = stored_models['resources']\n",
    "    for m in stored_models_details:\n",
    "        m_name = m['metadata']['name']\n",
    "        if m_name == model_name:\n",
    "            model_id = m['metadata']['id']\n",
    "            print(\"Deleteing model with id: \", model_id, \" and name: \", m_name)\n",
    "            wml_client.repository.delete(model_id)\n",
    "    return 'Success'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "afc1995cd31e4066902f695ecc3e84ab"
   },
   "outputs": [],
   "source": [
    "def deleteExistingDeploymentsSameName(wml_client,deployment_name):\n",
    "    stored_deployments=wml_client.deployments.get_details()\n",
    "    stored_deployment_details = stored_deployments['resources']\n",
    "    for d in stored_deployment_details:\n",
    "        d_name = d['metadata']['name']\n",
    "        if d_name == deployment_name:\n",
    "            deployment_id = d['metadata']['id']\n",
    "            print(\"Deleteing deployment with id: \", deployment_id, \" and name: \", d_name)\n",
    "            wml_client.deployments.delete(deployment_id)\n",
    "    return 'Success'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7d9ac4f07cb84ef8802c13d6012e6311"
   },
   "outputs": [],
   "source": [
    "# Delete Existing Deployments with same name\n",
    "deleteExistingDeploymentsSameName(wml_client,DEPLOYMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46daad63799346b68392c62df86c6dd1"
   },
   "outputs": [],
   "source": [
    "# Delete existing models with same name\n",
    "deleteExistingModelsSameName(wml_client,MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f211368d-f5c2-4918-8a7b-de0a0998cfd1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Storing model ...\")\n",
    "published_model_details = wml_client.repository.store_model(\n",
    "    model=model, \n",
    "    meta_props=model_props, \n",
    "    training_data=train_data, \n",
    "    pipeline=pipeline)\n",
    "\n",
    "model_uid = wml_client.repository.get_model_id(published_model_details)\n",
    "print(\"Done\")\n",
    "print(\"Model ID: {}\".format(model_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3d03a8fa-7776-46ae-891c-df2c8a0f92cb"
   },
   "outputs": [],
   "source": [
    "wml_client.repository.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ea53844f5174ad6817110c0483003f7"
   },
   "outputs": [],
   "source": [
    "wml_client.deployments.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9e7736e1-60b7-4c28-a307-3bea649ec3d3"
   },
   "source": [
    "## Deploy the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bfdd696-cd67-4a9f-b4e8-d17bcbad9a3f"
   },
   "source": [
    "The next section of the notebook deploys the model as a RESTful web service in Watson Machine Learning. The deployed model will have a scoring URL you can use to send data to the model for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6e0e6279-2acb-4d98-996f-eeb140536f8d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deployment_details = wml_client.deployments.create(\n",
    "    model_uid, \n",
    "    meta_props={\n",
    "        wml_client.deployments.ConfigurationMetaNames.NAME: \"{}\".format(DEPLOYMENT_NAME),\n",
    "        wml_client.deployments.ConfigurationMetaNames.ONLINE: {}\n",
    "    }\n",
    ")\n",
    "scoring_url = wml_client.deployments.get_scoring_href(deployment_details)\n",
    "deployment_uid=wml_client.deployments.get_uid(deployment_details)\n",
    "\n",
    "print(\"Scoring URL:\" + scoring_url)\n",
    "print(\"Model id: {}\".format(model_uid))\n",
    "print(\"Deployment id: {}\".format(deployment_uid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9f4a190b19bf47f48fd1fc5057f73066"
   },
   "source": [
    "## Sample scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37a1a1c1ee3b49c98633dc304b7e76f1"
   },
   "outputs": [],
   "source": [
    "fields = [\"ID\",\"LONGDISTANCE\",\"INTERNATIONAL\",\"LOCAL\",\"DROPPED\",\"PAYMETHOD\",\"LOCALBILLTYPE\",\"LONGDISTANCEBILLTYPE\",\"USAGE\",\\\n",
    "            \"RATEPLAN\",\"GENDER\",\"STATUS\",\"CHILDREN\",\"ESTINCOME\",\"CAROWNER\",\"AGE\"]\n",
    "values = [[1,28,0,60,0,\"Auto\",\"FreeLocal\",\"Standard\",89,4,\"F\",\"M\",1,23000,\"N\",45]]\n",
    "scoring_payload = {\"input_data\": [{\"fields\": fields, \"values\": values}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43fbce986afd41898d3f59de4e7fffd8"
   },
   "outputs": [],
   "source": [
    "scoring_response = wml_client.deployments.score(deployment_uid, scoring_payload)\n",
    "scoring_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8403ebbb429640509d2e81d18bc6ad6d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
